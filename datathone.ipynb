{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP0YkqhHLPFrWFqT82OQLGk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TalaQattan/tech-interview-handbook/blob/main/datathone.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"ðŸ“¦ Installing packages...\")\n",
        "# Install all necessary packages with specific versions in one go\n",
        "!pip install -q numpy==1.26.4 pmdarima xgboost lightgbm openpyxl pyarrow pandas==2.2.2\n",
        "\n",
        "print(\"âœ… Installation complete!\")"
      ],
      "metadata": {
        "id": "YhMBjkJECHQz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Import libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "import pmdarima as pmd\n",
        "from xgboost import XGBRegressor\n",
        "from lightgbm import LGBMRegressor\n",
        "\n",
        "# Linear models + scaling\n",
        "from sklearn.linear_model import Ridge, ElasticNet\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "print(\"Setup complete.\\n\")\n",
        "\n",
        "# ============================================\n",
        "# CONFIG: Initial settings (will auto-adjust)\n",
        "# ============================================\n",
        "TRAIN_WINDOW = 12   # Target: 12 months training window\n",
        "FH = 1              # Forecast horizon: 1 month ahead\n",
        "MAX_LAG = 6         # Lag depth for ML models\n",
        "\n",
        "# Files\n",
        "EXCEL_RAW = \"Claims.xlsx\"  # <-- Change this if needed\n",
        "CLEAN_PARQUET = \"claims_clean.parquet\"\n",
        "\n",
        "print(f\"Looking for: {EXCEL_RAW}\\n\")\n",
        "\n",
        "# ============================================\n",
        "# LOAD & CLEAN DATA\n",
        "# ============================================\n",
        "\n",
        "def parse_yyyymm(series):\n",
        "    \"\"\"Convert YYYYMM format to datetime\"\"\"\n",
        "    s = series.astype(str).str.extract(r\"(\\d{6})\", expand=False)\n",
        "    return pd.to_datetime(s, format=\"%Y%m\", errors=\"coerce\")\n",
        "\n",
        "# Load data\n",
        "print(\"Loading data...\")\n",
        "if Path(CLEAN_PARQUET).exists():\n",
        "    df = pd.read_parquet(CLEAN_PARQUET)  # fixed typo\n",
        "    print(\"Loaded from parquet\")\n",
        "else:\n",
        "    df = pd.read_excel(EXCEL_RAW, engine=\"openpyxl\")\n",
        "    print(f\"Loaded {len(df):,} rows from Excel\")\n",
        "\n",
        "# Make columns consistent\n",
        "df.columns = [str(c).strip().upper() for c in df.columns]\n",
        "\n",
        "# ----------------------------\n",
        "# Enforce TREATMENT <= BATCH\n",
        "# ----------------------------\n",
        "def to_dt(col):\n",
        "    return pd.to_datetime(\n",
        "        col.astype(str).str.extract(r\"(\\d{6})\", expand=False),\n",
        "        format=\"%Y%m\",\n",
        "        errors=\"coerce\"\n",
        "    )\n",
        "\n",
        "treat_dt = to_dt(df[\"TREATMENT_PERIOD\"]) if \"TREATMENT_PERIOD\" in df.columns else None\n",
        "batch_dt = to_dt(df[\"BATCH_PERIOD\"])     if \"BATCH_PERIOD\"     in df.columns else None\n",
        "\n",
        "if (treat_dt is not None) and (batch_dt is not None):\n",
        "    mask_valid = treat_dt.notna() & batch_dt.notna() & (treat_dt <= batch_dt)\n",
        "    kept = int(mask_valid.sum()); dropped = int((~mask_valid).sum())\n",
        "    print(f\"Order filter: kept {kept:,}, dropped {dropped:,} (invalid or TREATMENT > BATCH)\")\n",
        "    df = df.loc[mask_valid].copy()\n",
        "    df[\"_TREAT_DT\"] = treat_dt.loc[mask_valid]\n",
        "    df[\"_BATCH_DT\"] = batch_dt.loc[mask_valid]\n",
        "else:\n",
        "    print(\"Skipped order filter (one of the columns missing).\")\n",
        "\n",
        "# Filter to approved claims\n",
        "if \"STATUS\" in df.columns:\n",
        "    print(\"\\nAvailable status codes:\")\n",
        "    print(df[\"STATUS\"].value_counts().head(10))\n",
        "\n",
        "    df[\"STATUS\"] = df[\"STATUS\"].astype(str).str.upper()\n",
        "    approved_codes = [\"APPROVED\", \"AC\", \"PAID\"]\n",
        "    df = df[df[\"STATUS\"].isin(approved_codes)]\n",
        "    print(f\"\\nFiltered to {len(df):,} approved claims\")\n",
        "else:\n",
        "    print(\"No STATUS column found - using all rows\")\n",
        "\n",
        "# Ensure numeric positive amounts\n",
        "df[\"CLAIMS_AMOUNT\"] = pd.to_numeric(df.get(\"CLAIMS_AMOUNT\"), errors=\"coerce\")\n",
        "df = df[df[\"CLAIMS_AMOUNT\"] > 0]\n",
        "print(f\"{len(df):,} claims with positive amounts\\n\")\n",
        "\n",
        "# Monthly aggregation\n",
        "if \"_TREAT_DT\" in df.columns:\n",
        "    month_idx = df[\"_TREAT_DT\"]\n",
        "elif \"_BATCH_DT\" in df.columns:\n",
        "    month_idx = df[\"_BATCH_DT\"]\n",
        "elif \"TREATMENT_PERIOD\" in df.columns:\n",
        "    month_idx = parse_yyyymm(df[\"TREATMENT_PERIOD\"])\n",
        "elif \"BATCH_PERIOD\" in df.columns:\n",
        "    month_idx = parse_yyyymm(df[\"BATCH_PERIOD\"])\n",
        "else:\n",
        "    raise ValueError(\"Need TREATMENT_PERIOD or BATCH_PERIOD column (YYYYMM format)\")\n",
        "\n",
        "df = df.loc[month_idx.notna()].copy()\n",
        "df[\"_MONTH\"] = month_idx.dt.to_period(\"M\").dt.to_timestamp()\n",
        "\n",
        "monthly = (\n",
        "    df.groupby(\"_MONTH\", as_index=True)[\"CLAIMS_AMOUNT\"]\n",
        "      .sum()\n",
        "      .sort_index()\n",
        "      .asfreq(\"MS\", fill_value=0.0)\n",
        "      .to_frame(name=\"TOTAL_COST\")\n",
        ")\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"Monthly Data Summary\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Date range: {monthly.index.min().date()} to {monthly.index.max().date()}\")\n",
        "print(f\"Total months: {len(monthly)}\")\n",
        "print(f\"Total claims amount: ${monthly['TOTAL_COST'].sum():,.0f}\")\n",
        "print(f\"Average monthly cost: ${monthly['TOTAL_COST'].mean():,.0f}\")\n",
        "print(\"\\nLast 6 months:\")\n",
        "print(monthly.tail(6))\n",
        "\n",
        "# ============================================\n",
        "# AUTO-ADJUST CV PARAMETERS BASED ON DATA\n",
        "# ============================================\n",
        "total_months = len(monthly)\n",
        "y = monthly[\"TOTAL_COST\"].copy()\n",
        "X_exog = None  # No exogenous features\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"AUTO-ADJUSTING CV PARAMETERS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Determine optimal training window\n",
        "if total_months < 12:\n",
        "    TRAIN_WINDOW = max(6, total_months - 3)  # Use most data, leave room for CV\n",
        "    print(f\"Limited data: Adjusting TRAIN_WINDOW to {TRAIN_WINDOW} months\")\n",
        "elif total_months < 18:\n",
        "    TRAIN_WINDOW = 10\n",
        "    print(f\"Moderate data: Adjusting TRAIN_WINDOW to {TRAIN_WINDOW} months\")\n",
        "else:\n",
        "    TRAIN_WINDOW = 12\n",
        "    print(f\"Sufficient data: Using TRAIN_WINDOW = {TRAIN_WINDOW} months\")\n",
        "\n",
        "# Adjust MAX_LAG based on training window\n",
        "MAX_LAG = min(6, TRAIN_WINDOW // 2)\n",
        "print(f\"MAX_LAG adjusted to {MAX_LAG} (<= {TRAIN_WINDOW}//2)\")\n",
        "\n",
        "# Calculate maximum possible folds\n",
        "min_months_needed = TRAIN_WINDOW + FH\n",
        "max_possible_folds = total_months - TRAIN_WINDOW - FH + 1\n",
        "\n",
        "if total_months < min_months_needed:\n",
        "    raise ValueError(\n",
        "        f\"ERROR: Need at least {min_months_needed} months for CV\\n\"\n",
        "        f\"   You have: {total_months} months\\n\"\n",
        "        f\"   Options: 1) Get more data, 2) Reduce TRAIN_WINDOW, or 3) Skip CV\"\n",
        "    )\n",
        "\n",
        "# Determine optimal number of folds\n",
        "if max_possible_folds >= 5:\n",
        "    N_SPLITS = 5\n",
        "    print(f\"Using N_SPLITS = {N_SPLITS} folds (optimal)\")\n",
        "elif max_possible_folds >= 3:\n",
        "    N_SPLITS = max_possible_folds\n",
        "    print(f\"Using N_SPLITS = {N_SPLITS} folds (maximum possible)\")\n",
        "else:\n",
        "    N_SPLITS = max(1, max_possible_folds)\n",
        "    print(f\"Using N_SPLITS = {N_SPLITS} fold (very limited data)\")\n",
        "\n",
        "print(\"\\nFinal CV Configuration:\")\n",
        "print(f\"   - Training window: {TRAIN_WINDOW} months\")\n",
        "print(f\"   - Forecast horizon: {FH} month\")\n",
        "print(f\"   - CV folds: {N_SPLITS}\")\n",
        "print(f\"   - Max lag features: {MAX_LAG}\")\n",
        "print(f\"   - Total evaluations: {N_SPLITS} per model\")\n",
        "\n",
        "# ============================================\n",
        "# HELPER FUNCTIONS\n",
        "# ============================================\n",
        "\n",
        "def smape(y_true, y_pred):\n",
        "    \"\"\"Symmetric Mean Absolute Percentage Error (in %)\"\"\"\n",
        "    denom = (np.abs(y_true) + np.abs(y_pred)) / 2.0\n",
        "    mask = denom != 0\n",
        "    if mask.sum() == 0:\n",
        "        return 0.0\n",
        "    return np.mean(np.abs(y_true[mask] - y_pred[mask]) / denom[mask]) * 100.0\n",
        "\n",
        "def build_supervised_from_series(y_series, extra_X=None, max_lag=MAX_LAG):\n",
        "    \"\"\"Create lagged features and rolling stats\"\"\"\n",
        "    df_feat = pd.DataFrame(index=y_series.index)\n",
        "    df_feat[\"y\"] = y_series.values\n",
        "\n",
        "    # Lag features\n",
        "    for L in range(1, max_lag+1):\n",
        "        df_feat[f\"lag_{L}\"] = y_series.shift(L)\n",
        "\n",
        "    # Rolling statistics (adapt to training window)\n",
        "    roll_windows = [3, min(6, max_lag)]\n",
        "    for w in range(len(roll_windows)):\n",
        "        window = roll_windows[w]\n",
        "        df_feat[f\"roll_mean_{window}\"] = y_series.shift(1).rolling(window).mean()\n",
        "        df_feat[f\"roll_std_{window}\"]  = y_series.shift(1).rolling(window).std()\n",
        "\n",
        "    # Calendar features\n",
        "    df_feat[\"month\"] = df_feat.index.month\n",
        "    df_feat[\"quarter\"] = df_feat.index.quarter\n",
        "\n",
        "    if extra_X is not None:\n",
        "        df_feat = df_feat.join(extra_X, how=\"left\")\n",
        "\n",
        "    # NOTE: We do NOT dropna() here. NaNs will be handled during splitting/forecasting.\n",
        "    return df_feat.drop(columns=[\"y\"]), df_feat[\"y\"]\n",
        "\n",
        "def rolling_window_splits(y_index, window=TRAIN_WINDOW, fh=FH, n_splits=N_SPLITS):\n",
        "    \"\"\"\n",
        "    Fixed-length rolling window CV\n",
        "    Ensures all folds are valid and evenly spaced\n",
        "    \"\"\"\n",
        "    n = len(y_index)\n",
        "\n",
        "    # Ensure we have enough data\n",
        "    if n < window + fh:\n",
        "        raise ValueError(f\"Need at least {window + fh} months, have {n}\")\n",
        "\n",
        "    # Calculate valid range for test set end positions\n",
        "    first_test_end = window + fh\n",
        "    last_test_end = n\n",
        "\n",
        "    if n_splits == 1:\n",
        "        # Single fold: use the very last possible window\n",
        "        test_end_positions = [last_test_end]\n",
        "    else:\n",
        "        # Multiple folds: evenly space them\n",
        "        test_end_positions = np.linspace(first_test_end, last_test_end, n_splits, dtype=int)\n",
        "\n",
        "    for test_end in test_end_positions:\n",
        "        train_start = test_end - fh - window\n",
        "        train_end = test_end - fh\n",
        "        test_start = train_end\n",
        "        test_end_ = test_end\n",
        "\n",
        "        train_idx = np.arange(train_start, train_end)\n",
        "        test_idx  = np.arange(test_start, test_end_)\n",
        "\n",
        "        yield train_idx, test_idx\n",
        "\n",
        "# ============================================\n",
        "# MODEL EVALUATION\n",
        "# ============================================\n",
        "\n",
        "def evaluate_models_auto_cv(y_series, X_exog=None, window=TRAIN_WINDOW, fh=FH, n_splits=N_SPLITS):\n",
        "    \"\"\"Evaluate all models with auto-adjusted CV\"\"\"\n",
        "    results = []\n",
        "    idx = y_series.index\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"CROSS-VALIDATION\")\n",
        "    print(\"=\"*60)\n",
        "    print(f\"Configuration: {n_splits} folds, train={window} months, forecast={fh} month\\n\")\n",
        "\n",
        "    # Pre-build full feature table - don't dropna here\n",
        "    X_full, y_full = build_supervised_from_series(y_series, extra_X=X_exog, max_lag=MAX_LAG)\n",
        "\n",
        "    # Add baseline (12-month average) for comparison\n",
        "    print(\"Models to evaluate:\")\n",
        "    print(\"  - Baseline (12-month moving average)\")\n",
        "    print(\"  - ARIMA\")\n",
        "    print(\"  - Ridge Regression\")\n",
        "    print(\"  - ElasticNet\")\n",
        "    print(\"  - XGBoost\")\n",
        "    print(\"  - LightGBM\\n\")\n",
        "\n",
        "    for fold, (tr, te) in enumerate(rolling_window_splits(idx, window=window, fh=fh, n_splits=n_splits), start=1):\n",
        "        train_dates = f\"{idx[tr[0]].date()} to {idx[tr[-1]].date()}\"\n",
        "        test_date = idx[te[0]].date()\n",
        "        print(f\"Fold {fold}/{n_splits}: Train [{train_dates}] -> Test [{test_date}]\")\n",
        "\n",
        "        y_tr = y_series.iloc[tr]\n",
        "        y_te = y_series.iloc[te]\n",
        "\n",
        "        # -----------------------\n",
        "        # BASELINE: 12-month (or window) average\n",
        "        # -----------------------\n",
        "        baseline_window = min(12, len(y_tr))\n",
        "        baseline_pred = y_tr.iloc[-baseline_window:].mean()\n",
        "        results.append([\"Baseline_MA\", fold,\n",
        "                       float(np.sqrt((y_te.values[0] - baseline_pred)**2)),\n",
        "                       float(np.abs(y_te.values[0] - baseline_pred)),\n",
        "                       float(smape(y_te.values, np.array([baseline_pred])))])\n",
        "\n",
        "        # -----------------------\n",
        "        # ARIMA\n",
        "        # -----------------------\n",
        "        try:\n",
        "            arima = pmd.auto_arima(y_tr, seasonal=False, stepwise=True,\n",
        "                                   suppress_warnings=True, error_action=\"ignore\",\n",
        "                                   max_p=3, max_q=3)  # Limit complexity for small data\n",
        "            y_pred = arima.predict(n_periods=fh)[0]\n",
        "            results.append([\"ARIMA\", fold,\n",
        "                            float(np.sqrt((y_te.values[0] - y_pred)**2)),\n",
        "                            float(np.abs(y_te.values[0] - y_pred)),\n",
        "                            float(smape(y_te.values, np.array([y_pred])))])\n",
        "        except Exception:\n",
        "            results.append([\"ARIMA\", fold, np.nan, np.nan, np.nan])\n",
        "\n",
        "        # -----------------------\n",
        "        # ML MODELS (with lag features)\n",
        "        # -----------------------\n",
        "        # Build once outside the loop (already done), slice here\n",
        "        X_tr = X_full.iloc[tr].dropna()                 # Drop NaNs for training\n",
        "        y_tr_full = y_full.iloc[tr].loc[X_tr.index]     # Align y_full with X_tr after dropping NaNs\n",
        "        X_te = X_full.iloc[te]                          # Keep NaNs in test for prediction\n",
        "\n",
        "        # Check if we have enough training data for ML models\n",
        "        min_ml_rows = max(5, MAX_LAG)\n",
        "\n",
        "        if len(X_te) >= 1 and len(X_tr) >= min_ml_rows:\n",
        "            # Ridge\n",
        "            try:\n",
        "                ridge = Pipeline([\n",
        "                    (\"scaler\", StandardScaler(with_mean=True, with_std=True)),\n",
        "                    (\"model\", Ridge(alpha=1.0))\n",
        "                ])\n",
        "                ridge.fit(X_tr, y_tr_full)\n",
        "                y_pred = ridge.predict(X_te)[0]\n",
        "                results.append([\"Ridge\", fold,\n",
        "                                float(np.sqrt((y_te.values[0] - y_pred)**2)),\n",
        "                                float(np.abs(y_te.values[0] - y_pred)),\n",
        "                                float(smape(y_te.values, np.array([y_pred])))])\n",
        "            except Exception:\n",
        "                results.append([\"Ridge\", fold, np.nan, np.nan, np.nan])\n",
        "\n",
        "            # ElasticNet\n",
        "            try:\n",
        "                enet = Pipeline([\n",
        "                    (\"scaler\", StandardScaler(with_mean=True, with_std=True)),\n",
        "                    (\"model\", ElasticNet(alpha=0.1, l1_ratio=0.5, random_state=42, max_iter=10000))\n",
        "                ])\n",
        "                enet.fit(X_tr, y_tr_full)\n",
        "                y_pred = enet.predict(X_te)[0]\n",
        "                results.append([\"ElasticNet\", fold,\n",
        "                                float(np.sqrt((y_te.values[0] - y_pred)**2)),\n",
        "                                float(np.abs(y_te.values[0] - y_pred)),\n",
        "                                float(smape(y_te.values, np.array([y_pred])))])\n",
        "            except Exception:\n",
        "                results.append([\"ElasticNet\", fold, np.nan, np.nan, np.nan])\n",
        "\n",
        "            # XGBoost\n",
        "            try:\n",
        "                xgb = XGBRegressor(n_estimators=300, max_depth=3, learning_rate=0.05,\n",
        "                                   subsample=0.9, colsample_bytree=0.9, random_state=42,\n",
        "                                   verbosity=0)\n",
        "                xgb.fit(X_tr, y_tr_full)\n",
        "                y_pred = xgb.predict(X_te)[0]\n",
        "                results.append([\"XGBoost\", fold,\n",
        "                                float(np.sqrt((y_te.values[0] - y_pred)**2)),\n",
        "                                float(np.abs(y_te.values[0] - y_pred)),\n",
        "                                float(smape(y_te.values, np.array([y_pred])))])\n",
        "            except Exception:\n",
        "                results.append([\"XGBoost\", fold, np.nan, np.nan, np.nan])\n",
        "\n",
        "            # LightGBM\n",
        "            try:\n",
        "                lgbm = LGBMRegressor(n_estimators=300, num_leaves=15, learning_rate=0.05,\n",
        "                                     subsample=0.9, colsample_bytree=0.9, random_state=42,\n",
        "                                     verbose=-1)\n",
        "                lgbm.fit(X_tr, y_tr_full)\n",
        "                y_pred = lgbm.predict(X_te)[0]\n",
        "                results.append([\"LightGBM\", fold,\n",
        "                                float(np.sqrt((y_te.values[0] - y_pred)**2)),\n",
        "                                float(np.abs(y_te.values[0] - y_pred)),\n",
        "                                float(smape(y_te.values, np.array([y_pred])))])\n",
        "            except Exception:\n",
        "                results.append([\"LightGBM\", fold, np.nan, np.nan, np.nan])\n",
        "        else:\n",
        "            # Not enough rows for ML\n",
        "            for name in [\"Ridge\", \"ElasticNet\", \"XGBoost\", \"LightGBM\"]:\n",
        "                results.append([name, fold, np.nan, np.nan, np.nan])\n",
        "\n",
        "    res_df = pd.DataFrame(results, columns=[\"Model\",\"Fold\",\"RMSE\",\"MAE\",\"sMAPE\"])\n",
        "\n",
        "    # Calculate average metrics, handling NaNs\n",
        "    leaderboard = (res_df.groupby(\"Model\", as_index=True)\n",
        "                        .agg(RMSE=(\"RMSE\",\"mean\"),\n",
        "                             MAE=(\"MAE\",\"mean\"),\n",
        "                             sMAPE=(\"sMAPE\",\"mean\"),\n",
        "                             Valid_Folds=(\"RMSE\", lambda x: x.notna().sum()))\n",
        "                        .sort_values(\"RMSE\"))\n",
        "\n",
        "    # ============================================\n",
        "    # ADD: Accuracy % from RMSE + MAE in SAR\n",
        "    # ============================================\n",
        "    target_range = float(y_series.max() - y_series.min())\n",
        "    leaderboard = leaderboard.copy()\n",
        "    if target_range > 0 and np.isfinite(target_range):\n",
        "        leaderboard[\"Accuracy_%\"] = (1.0 - (leaderboard[\"RMSE\"] / target_range)) * 100.0\n",
        "        leaderboard[\"Accuracy_%\"] = leaderboard[\"Accuracy_%\"].clip(lower=-100, upper=100).round(2)\n",
        "    else:\n",
        "        leaderboard[\"Accuracy_%\"] = np.nan\n",
        "        print(\"Warning: Accuracy_% not computed because target_range is zero or invalid.\")\n",
        "\n",
        "    leaderboard[\"MAE_SAR\"] = leaderboard[\"MAE\"].round(0)\n",
        "\n",
        "    # Optional: reorder columns for readability\n",
        "    cols_order = [\"RMSE\", \"MAE_SAR\", \"sMAPE\", \"Accuracy_%\", \"Valid_Folds\"]\n",
        "    leaderboard = leaderboard.reindex(columns=[c for c in cols_order if c in leaderboard.columns])\n",
        "\n",
        "    return res_df, leaderboard\n",
        "\n",
        "# ============================================\n",
        "# RUN EVALUATION\n",
        "# ============================================\n",
        "\n",
        "cv_results, leaderboard = evaluate_models_auto_cv(y, X_exog=X_exog,\n",
        "                                                  window=TRAIN_WINDOW, fh=FH, n_splits=N_SPLITS)\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"FINAL LEADERBOARD (Lower is Better for RMSE/MAE/sMAPE; Higher is Better for Accuracy_%)\")\n",
        "print(\"=\"*60)\n",
        "print(leaderboard.to_string())\n",
        "print(\"\\n\")\n",
        "\n",
        "# Show which models completed successfully\n",
        "valid_models = leaderboard[leaderboard['Valid_Folds'] == N_SPLITS]\n",
        "if len(valid_models) > 0:\n",
        "    print(f\"Models with all {N_SPLITS} successful folds:\")\n",
        "    print(f\"   {', '.join(valid_models.index.tolist())}\")\n",
        "else:\n",
        "    print(f\"No models completed all {N_SPLITS} folds successfully\")\n",
        "\n",
        "# ============================================\n",
        "# FEATURE IMPORTANCE (if winner is ML model)\n",
        "# ============================================\n",
        "\n",
        "best_model = leaderboard.index[0]\n",
        "if best_model in [\"XGBoost\", \"LightGBM\", \"Ridge\", \"ElasticNet\"]:\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(f\"FEATURE IMPORTANCE FOR {best_model}\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    # Re-build full feature table - don't dropna here\n",
        "    X_full, y_full = build_supervised_from_series(y, extra_X=X_exog, max_lag=MAX_LAG)\n",
        "\n",
        "    # Use last training window for feature importance, dropna for training\n",
        "    last_window_idx = y.index[-TRAIN_WINDOW:]\n",
        "    X_tr = X_full.loc[last_window_idx].dropna()\n",
        "    y_tr = y_full.loc[X_tr.index]  # Align y_full with X_tr\n",
        "\n",
        "    if best_model == \"XGBoost\":\n",
        "        model = XGBRegressor(n_estimators=300, max_depth=3, learning_rate=0.05,\n",
        "                             subsample=0.9, colsample_bytree=0.9, random_state=42, verbosity=0)\n",
        "    elif best_model == \"LightGBM\":\n",
        "        model = LGBMRegressor(n_estimators=300, num_leaves=15, learning_rate=0.05,\n",
        "                              subsample=0.9, colsample_bytree=0.9, random_state=42, verbose=-1)\n",
        "    elif best_model == \"Ridge\":\n",
        "        model = Pipeline([\n",
        "            (\"scaler\", StandardScaler(with_mean=True, with_std=True)),\n",
        "            (\"model\", Ridge(alpha=1.0))\n",
        "        ])\n",
        "    else:  # ElasticNet\n",
        "        model = Pipeline([\n",
        "            (\"scaler\", StandardScaler(with_mean=True, with_std=True)),\n",
        "            (\"model\", ElasticNet(alpha=0.1, l1_ratio=0.5, random_state=42, max_iter=10000))\n",
        "        ])\n",
        "\n",
        "    model.fit(X_tr, y_tr)\n",
        "\n",
        "    # Extract feature importance\n",
        "    if best_model in [\"XGBoost\", \"LightGBM\"]:\n",
        "        feat_imp = pd.DataFrame({\n",
        "            'Feature': X_tr.columns,\n",
        "            'Importance': model.feature_importances_\n",
        "        }).sort_values('Importance', ascending=False)\n",
        "    else:  # Ridge or ElasticNet\n",
        "        feat_imp = pd.DataFrame({\n",
        "            'Feature': X_tr.columns,\n",
        "            'Coefficient': np.abs(model.named_steps['model'].coef_)\n",
        "        }).sort_values('Coefficient', ascending=False)\n",
        "        feat_imp.columns = ['Feature', 'Importance']\n",
        "\n",
        "    print(\"\\nTop 10 Most Important Features:\")\n",
        "    print(feat_imp.head(10).to_string(index=False))\n",
        "\n",
        "# ============================================\n",
        "# FINAL PREDICTION\n",
        "# ============================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(f\"FINAL FORECAST USING: {best_model}\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "last_window_idx = y.index[-TRAIN_WINDOW:]\n",
        "y_train_last = y.loc[last_window_idx]\n",
        "next_month = (y.index[-1] + pd.offsets.MonthBegin(1))\n",
        "\n",
        "if best_model == \"Baseline_MA\":\n",
        "    baseline_window = min(12, len(y))\n",
        "    yhat = float(y.iloc[-baseline_window:].mean())\n",
        "\n",
        "elif best_model == \"ARIMA\":\n",
        "    mdl = pmd.auto_arima(y_train_last, seasonal=False, stepwise=True,\n",
        "                         suppress_warnings=True, error_action=\"ignore\",\n",
        "                         max_p=3, max_q=3)\n",
        "    yhat = float(mdl.predict(n_periods=FH)[0])\n",
        "\n",
        "else:  # ML models\n",
        "    # Build features for the entire series (including the forecast month as NaN)\n",
        "    y_tmp = y.copy()\n",
        "    y_tmp.loc[next_month] = np.nan\n",
        "    X_full_for_pred, _ = build_supervised_from_series(y_tmp, extra_X=X_exog, max_lag=MAX_LAG)\n",
        "\n",
        "    # Select training data (dropna for training)\n",
        "    X_tr = X_full_for_pred.loc[last_window_idx].dropna()\n",
        "    y_tr_full = y.loc[X_tr.index]  # y is the original series without the NaN forecast month\n",
        "\n",
        "    # Select the features for the next month (do NOT drop NaNs here)\n",
        "    X_next = X_full_for_pred.loc[[next_month]]\n",
        "\n",
        "    if best_model == \"Ridge\":\n",
        "        model = Pipeline([\n",
        "            (\"scaler\", StandardScaler(with_mean=True, with_std=True)),\n",
        "            (\"model\", Ridge(alpha=1.0))\n",
        "        ])\n",
        "    elif best_model == \"ElasticNet\":\n",
        "        model = Pipeline([\n",
        "            (\"scaler\", StandardScaler(with_mean=True, with_std=True)),\n",
        "            (\"model\", ElasticNet(alpha=0.1, l1_ratio=0.5, random_state=42, max_iter=10000))\n",
        "        ])\n",
        "    elif best_model == \"XGBoost\":\n",
        "        model = XGBRegressor(n_estimators=300, max_depth=3, learning_rate=0.05,\n",
        "                             subsample=0.9, colsample_bytree=0.9, random_state=42, verbosity=0)\n",
        "    else:  # LightGBM\n",
        "        model = LGBMRegressor(n_estimators=300, num_leaves=15, learning_rate=0.05,\n",
        "                              subsample=0.9, colsample_bytree=0.9, random_state=42, verbose=-1)\n",
        "\n",
        "    model.fit(X_tr, y_tr_full)\n",
        "    yhat = float(model.predict(X_next)[0])\n",
        "\n",
        "forecast = pd.DataFrame({\n",
        "    \"Month\": [next_month.strftime('%Y-%m')],\n",
        "    \"Predicted_Cost\": [f\"${yhat:,.2f}\"],\n",
        "    \"Model_Used\": [best_model],\n",
        "    \"Training_Window\": [f\"{y_train_last.index[0].strftime('%Y-%m')} to {y_train_last.index[-1].strftime('%Y-%m')}\"]\n",
        "})\n",
        "\n",
        "print(f\"\\nForecast for {next_month.strftime('%B %Y')}:\")\n",
        "print(forecast.to_string(index=False))\n",
        "\n",
        "# ============================================\n",
        "# SAVE OUTPUTS\n",
        "# ============================================\n",
        "\n",
        "monthly.to_csv(\"monthly_approved_cost.csv\")\n",
        "forecast_save = pd.DataFrame({\n",
        "    \"Month\": [next_month],\n",
        "    \"Predicted_Cost\": [yhat],\n",
        "    \"Model_Used\": [best_model],\n",
        "    \"CV_Folds\": [N_SPLITS],\n",
        "    \"Training_Window_Months\": [TRAIN_WINDOW]\n",
        "})\n",
        "forecast_save.to_csv(\"next_month_forecast.csv\", index=False)\n",
        "\n",
        "# Save detailed CV results\n",
        "cv_results.to_csv(\"cv_results_detailed.csv\", index=False)\n",
        "leaderboard.to_csv(\"model_leaderboard.csv\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"SAVED FILES:\")\n",
        "print(\"  monthly_approved_cost.csv\")\n",
        "print(\"  next_month_forecast.csv\")\n",
        "print(\"  cv_results_detailed.csv\")\n",
        "print(\"  model_leaderboard.csv\")\n",
        "print(\"=\"*60)\n",
        "print(\"\\nAnalysis complete.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FBebnqPb8AjF",
        "outputId": "7f32a6e8-2eab-43c0-9b3e-3e51b4d73cac"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setup complete.\n",
            "\n",
            "Looking for: Claims.xlsx\n",
            "\n",
            "Loading data...\n",
            "Loaded 900,000 rows from Excel\n",
            "Order filter: kept 899,510, dropped 490 (invalid or TREATMENT > BATCH)\n",
            "\n",
            "Available status codes:\n",
            "STATUS\n",
            "AC    667473\n",
            "RJ    231888\n",
            "SU       149\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Filtered to 667,473 approved claims\n",
            "667,473 claims with positive amounts\n",
            "\n",
            "============================================================\n",
            "Monthly Data Summary\n",
            "============================================================\n",
            "Date range: 2023-01-01 to 2023-12-01\n",
            "Total months: 12\n",
            "Total claims amount: $206,628,283\n",
            "Average monthly cost: $17,219,024\n",
            "\n",
            "Last 6 months:\n",
            "             TOTAL_COST\n",
            "_MONTH                 \n",
            "2023-07-01  16205185.07\n",
            "2023-08-01  16696400.64\n",
            "2023-09-01  16917400.96\n",
            "2023-10-01  18769256.95\n",
            "2023-11-01  17215728.24\n",
            "2023-12-01  18341687.91\n",
            "\n",
            "============================================================\n",
            "AUTO-ADJUSTING CV PARAMETERS\n",
            "============================================================\n",
            "Moderate data: Adjusting TRAIN_WINDOW to 10 months\n",
            "MAX_LAG adjusted to 5 (<= 10//2)\n",
            "Using N_SPLITS = 2 fold (very limited data)\n",
            "\n",
            "Final CV Configuration:\n",
            "   - Training window: 10 months\n",
            "   - Forecast horizon: 1 month\n",
            "   - CV folds: 2\n",
            "   - Max lag features: 5\n",
            "   - Total evaluations: 2 per model\n",
            "\n",
            "============================================================\n",
            "CROSS-VALIDATION\n",
            "============================================================\n",
            "Configuration: 2 folds, train=10 months, forecast=1 month\n",
            "\n",
            "Models to evaluate:\n",
            "  - Baseline (12-month moving average)\n",
            "  - ARIMA\n",
            "  - Ridge Regression\n",
            "  - ElasticNet\n",
            "  - XGBoost\n",
            "  - LightGBM\n",
            "\n",
            "Fold 1/2: Train [2023-01-01 to 2023-10-01] -> Test [2023-11-01]\n",
            "Fold 2/2: Train [2023-02-01 to 2023-11-01] -> Test [2023-12-01]\n",
            "\n",
            "============================================================\n",
            "FINAL LEADERBOARD (Lower is Better for RMSE/MAE/sMAPE; Higher is Better for Accuracy_%)\n",
            "============================================================\n",
            "                     RMSE    MAE_SAR     sMAPE  Accuracy_%  Valid_Folds\n",
            "Model                                                                  \n",
            "XGBoost      1.714911e+05   171491.0  0.960042       97.29            2\n",
            "Baseline_MA  6.884605e+05   688461.0  3.897712       89.13            2\n",
            "LightGBM     8.279052e+05   827905.0  4.716621       86.93            2\n",
            "Ridge        1.195079e+06  1195079.0  6.659374       81.14            2\n",
            "ARIMA        1.229585e+06  1229585.0  7.145923       80.59            2\n",
            "ElasticNet   1.280819e+06  1280819.0  7.130833       79.78            2\n",
            "\n",
            "\n",
            "Models with all 2 successful folds:\n",
            "   XGBoost, Baseline_MA, LightGBM, Ridge, ARIMA, ElasticNet\n",
            "\n",
            "============================================================\n",
            "FEATURE IMPORTANCE FOR XGBoost\n",
            "============================================================\n",
            "\n",
            "Top 10 Most Important Features:\n",
            "    Feature  Importance\n",
            "      month    0.479653\n",
            " roll_std_5    0.171327\n",
            "      lag_4    0.149981\n",
            " roll_std_3    0.084195\n",
            "      lag_5    0.048333\n",
            "roll_mean_3    0.043258\n",
            "      lag_1    0.014469\n",
            "    quarter    0.007098\n",
            "      lag_2    0.001475\n",
            "      lag_3    0.000212\n",
            "\n",
            "============================================================\n",
            "FINAL FORECAST USING: XGBoost\n",
            "============================================================\n",
            "\n",
            "Forecast for January 2024:\n",
            "  Month Predicted_Cost Model_Used    Training_Window\n",
            "2024-01 $17,966,530.00    XGBoost 2023-03 to 2023-12\n",
            "\n",
            "============================================================\n",
            "SAVED FILES:\n",
            "  monthly_approved_cost.csv\n",
            "  next_month_forecast.csv\n",
            "  cv_results_detailed.csv\n",
            "  model_leaderboard.csv\n",
            "============================================================\n",
            "\n",
            "Analysis complete.\n"
          ]
        }
      ]
    }
  ]
}