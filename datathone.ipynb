{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMJURN+mafMitXXxvxZiauo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TalaQattan/tech-interview-handbook/blob/main/datathone.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"üì¶ Installing packages...\")\n",
        "!pip uninstall -y numpy pmdarima xgboost lightgbm openpyxl pyarrow\n",
        "!pip install -q numpy==1.26.4 pmdarima xgboost lightgbm openpyxl pyarrow\n",
        "\n",
        "# 2. Import libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "import pmdarima as pmd\n",
        "from xgboost import XGBRegressor\n",
        "from lightgbm import LGBMRegressor\n",
        "\n",
        "print(\"‚úÖ Setup complete!\\n\")"
      ],
      "metadata": {
        "id": "YhMBjkJECHQz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "afb2adc4-5a68-4375-eda3-4828011a8126"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üì¶ Installing packages...\n",
            "Found existing installation: numpy 1.26.4\n",
            "Uninstalling numpy-1.26.4:\n",
            "  Successfully uninstalled numpy-1.26.4\n",
            "Found existing installation: pmdarima 2.0.4\n",
            "Uninstalling pmdarima-2.0.4:\n",
            "  Successfully uninstalled pmdarima-2.0.4\n",
            "Found existing installation: xgboost 3.0.5\n",
            "Uninstalling xgboost-3.0.5:\n",
            "  Successfully uninstalled xgboost-3.0.5\n",
            "Found existing installation: lightgbm 4.6.0\n",
            "Uninstalling lightgbm-4.6.0:\n",
            "  Successfully uninstalled lightgbm-4.6.0\n",
            "Found existing installation: openpyxl 3.1.5\n",
            "Uninstalling openpyxl-3.1.5:\n",
            "  Successfully uninstalled openpyxl-3.1.5\n",
            "Found existing installation: pyarrow 21.0.0\n",
            "Uninstalling pyarrow-21.0.0:\n",
            "  Successfully uninstalled pyarrow-21.0.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "pylibcudf-cu12 25.6.0 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 21.0.0 which is incompatible.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\n",
            "cudf-cu12 25.6.0 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 21.0.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m‚úÖ Setup complete!\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bb309c60"
      },
      "source": [
        "# Restart the runtime to clear potential conflicts\n",
        "import os\n",
        "os.kill(os.getpid(), 9)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Import libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "import pmdarima as pmd\n",
        "from xgboost import XGBRegressor\n",
        "from lightgbm import LGBMRegressor\n",
        "\n",
        "# Linear models + scaling\n",
        "from sklearn.linear_model import Ridge, ElasticNet\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "print(\"‚úÖ Setup complete!\\n\")\n",
        "\n",
        "# ============================================\n",
        "# CONFIG: Initial settings (will auto-adjust)\n",
        "# ============================================\n",
        "TRAIN_WINDOW = 12   # Target: 12 months training window\n",
        "FH = 1              # Forecast horizon: 1 month ahead\n",
        "MAX_LAG = 6         # Lag depth for ML models\n",
        "\n",
        "# Files\n",
        "EXCEL_RAW = \"Claims.xlsx\"  # <-- Change this if needed\n",
        "CLEAN_PARQUET = \"claims_clean.parquet\"\n",
        "\n",
        "print(f\"üìÇ Looking for: {EXCEL_RAW}\\n\")\n",
        "\n",
        "# ============================================\n",
        "# LOAD & CLEAN DATA\n",
        "# ============================================\n",
        "\n",
        "def parse_yyyymm(series):\n",
        "    \"\"\"Convert YYYYMM format to datetime\"\"\"\n",
        "    s = series.astype(str).str.extract(r\"(\\d{6})\", expand=False)\n",
        "    return pd.to_datetime(s, format=\"%Y%m\", errors=\"coerce\")\n",
        "\n",
        "# Load data\n",
        "print(\"üìä Loading data...\")\n",
        "if Path(CLEAN_PARQUET).exists():\n",
        "    df = pd.read_parquet(CLEQUET)\n",
        "    print(\"‚úÖ Loaded from parquet\")\n",
        "else:\n",
        "    df = pd.read_excel(EXCEL_RAW, engine=\"openpyxl\")\n",
        "    print(f\"‚úÖ Loaded {len(df):,} rows from Excel\")\n",
        "\n",
        "# Make columns consistent\n",
        "df.columns = [str(c).strip().upper() for c in df.columns]\n",
        "\n",
        "# ----------------------------\n",
        "# Enforce TREATMENT <= BATCH\n",
        "# ----------------------------\n",
        "def to_dt(col):\n",
        "    return pd.to_datetime(\n",
        "        col.astype(str).str.extract(r\"(\\d{6})\", expand=False),\n",
        "        format=\"%Y%m\",\n",
        "        errors=\"coerce\"\n",
        "    )\n",
        "\n",
        "treat_dt = to_dt(df[\"TREATMENT_PERIOD\"]) if \"TREATMENT_PERIOD\" in df.columns else None\n",
        "batch_dt = to_dt(df[\"BATCH_PERIOD\"])     if \"BATCH_PERIOD\"     in df.columns else None\n",
        "\n",
        "if (treat_dt is not None) and (batch_dt is not None):\n",
        "    mask_valid = treat_dt.notna() & batch_dt.notna() & (treat_dt <= batch_dt)\n",
        "    kept = int(mask_valid.sum()); dropped = int((~mask_valid).sum())\n",
        "    print(f\"‚úÖ Order filter: kept {kept:,}, dropped {dropped:,} (invalid or TREATMENT > BATCH)\")\n",
        "    df = df.loc[mask_valid].copy()\n",
        "    df[\"_TREAT_DT\"] = treat_dt.loc[mask_valid]\n",
        "    df[\"_BATCH_DT\"] = batch_dt.loc[mask_valid]\n",
        "else:\n",
        "    print(\"‚ÑπÔ∏è Skipped order filter (one of the columns missing).\")\n",
        "\n",
        "# Filter to approved claims\n",
        "if \"STATUS\" in df.columns:\n",
        "    print(\"\\nüîç Available status codes:\")\n",
        "    print(df[\"STATUS\"].value_counts().head(10))\n",
        "\n",
        "    df[\"STATUS\"] = df[\"STATUS\"].astype(str).str.upper()\n",
        "    approved_codes = [\"APPROVED\", \"AC\", \"PAID\"]\n",
        "    df = df[df[\"STATUS\"].isin(approved_codes)]\n",
        "    print(f\"\\n‚úÖ Filtered to {len(df):,} approved claims\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è No STATUS column found - using all rows\")\n",
        "\n",
        "# Ensure numeric positive amounts\n",
        "df[\"CLAIMS_AMOUNT\"] = pd.to_numeric(df.get(\"CLAIMS_AMOUNT\"), errors=\"coerce\")\n",
        "df = df[df[\"CLAIMS_AMOUNT\"] > 0]\n",
        "print(f\"‚úÖ {len(df):,} claims with positive amounts\\n\")\n",
        "\n",
        "# Monthly aggregation\n",
        "if \"_TREAT_DT\" in df.columns:\n",
        "    month_idx = df[\"_TREAT_DT\"]\n",
        "elif \"_BATCH_DT\" in df.columns:\n",
        "    month_idx = df[\"_BATCH_DT\"]\n",
        "elif \"TREATMENT_PERIOD\" in df.columns:\n",
        "    month_idx = parse_yyyymm(df[\"TREATMENT_PERIOD\"])\n",
        "elif \"BATCH_PERIOD\" in df.columns:\n",
        "    month_idx = parse_yyyymm(df[\"BATCH_PERIOD\"])\n",
        "else:\n",
        "    raise ValueError(\"‚ùå Need TREATMENT_PERIOD or BATCH_PERIOD column (YYYYMM format)\")\n",
        "\n",
        "df = df.loc[month_idx.notna()].copy()\n",
        "df[\"_MONTH\"] = month_idx.dt.to_period(\"M\").dt.to_timestamp()\n",
        "\n",
        "monthly = (\n",
        "    df.groupby(\"_MONTH\", as_index=True)[\"CLAIMS_AMOUNT\"]\n",
        "      .sum()\n",
        "      .sort_index()\n",
        "      .asfreq(\"MS\", fill_value=0.0)\n",
        "      .to_frame(name=\"TOTAL_COST\")\n",
        ")\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(f\"üìÖ Monthly Data Summary\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Date range: {monthly.index.min().date()} ‚Üí {monthly.index.max().date()}\")\n",
        "print(f\"Total months: {len(monthly)}\")\n",
        "print(f\"Total claims amount: ${monthly['TOTAL_COST'].sum():,.0f}\")\n",
        "print(f\"Average monthly cost: ${monthly['TOTAL_COST'].mean():,.0f}\")\n",
        "print(f\"\\nLast 6 months:\")\n",
        "print(monthly.tail(6))\n",
        "\n",
        "# ============================================\n",
        "# AUTO-ADJUST CV PARAMETERS BASED ON DATA\n",
        "# ============================================\n",
        "total_months = len(monthly)\n",
        "y = monthly[\"TOTAL_COST\"].copy()\n",
        "X_exog = None  # No exogenous features\n",
        "\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(\"üîß AUTO-ADJUSTING CV PARAMETERS\")\n",
        "print(f\"{'='*60}\")\n",
        "\n",
        "# Determine optimal training window\n",
        "if total_months < 12:\n",
        "    TRAIN_WINDOW = max(6, total_months - 3)  # Use most data, leave room for CV\n",
        "    print(f\"‚ö†Ô∏è Limited data: Adjusting TRAIN_WINDOW to {TRAIN_WINDOW} months\")\n",
        "elif total_months < 18:\n",
        "    TRAIN_WINDOW = 10\n",
        "    print(f\"‚ÑπÔ∏è Moderate data: Adjusting TRAIN_WINDOW to {TRAIN_WINDOW} months\")\n",
        "else:\n",
        "    TRAIN_WINDOW = 12\n",
        "    print(f\"‚úÖ Sufficient data: Using TRAIN_WINDOW = {TRAIN_WINDOW} months\")\n",
        "\n",
        "# Adjust MAX_LAG based on training window\n",
        "MAX_LAG = min(6, TRAIN_WINDOW // 2)\n",
        "print(f\"‚úÖ MAX_LAG adjusted to {MAX_LAG} (‚â§ {TRAIN_WINDOW}//2)\")\n",
        "\n",
        "# Calculate maximum possible folds\n",
        "min_months_needed = TRAIN_WINDOW + FH\n",
        "max_possible_folds = total_months - TRAIN_WINDOW - FH + 1\n",
        "\n",
        "if total_months < min_months_needed:\n",
        "    raise ValueError(\n",
        "        f\"‚ùå ERROR: Need at least {min_months_needed} months for CV\\n\"\n",
        "        f\"   You have: {total_months} months\\n\"\n",
        "        f\"   Options: 1) Get more data, 2) Reduce TRAIN_WINDOW, or 3) Skip CV\"\n",
        "    )\n",
        "\n",
        "# Determine optimal number of folds\n",
        "if max_possible_folds >= 5:\n",
        "    N_SPLITS = 5\n",
        "    print(f\"‚úÖ Using N_SPLITS = {N_SPLITS} folds (optimal)\")\n",
        "elif max_possible_folds >= 3:\n",
        "    N_SPLITS = max_possible_folds\n",
        "    print(f\"‚ÑπÔ∏è Using N_SPLITS = {N_SPLITS} folds (maximum possible)\")\n",
        "else:\n",
        "    N_SPLITS = max(1, max_possible_folds)\n",
        "    print(f\"‚ö†Ô∏è Using N_SPLITS = {N_SPLITS} fold (very limited data)\")\n",
        "\n",
        "print(f\"\\nüìä Final CV Configuration:\")\n",
        "print(f\"   ‚Ä¢ Training window: {TRAIN_WINDOW} months\")\n",
        "print(f\"   ‚Ä¢ Forecast horizon: {FH} month\")\n",
        "print(f\"   ‚Ä¢ CV folds: {N_SPLITS}\")\n",
        "print(f\"   ‚Ä¢ Max lag features: {MAX_LAG}\")\n",
        "print(f\"   ‚Ä¢ Total evaluations: {N_SPLITS} per model\")\n",
        "\n",
        "# ============================================\n",
        "# HELPER FUNCTIONS\n",
        "# ============================================\n",
        "\n",
        "def smape(y_true, y_pred):\n",
        "    \"\"\"Symmetric Mean Absolute Percentage Error (in %)\"\"\"\n",
        "    denom = (np.abs(y_true) + np.abs(y_pred)) / 2.0\n",
        "    mask = denom != 0\n",
        "    if mask.sum() == 0:\n",
        "        return 0.0\n",
        "    return np.mean(np.abs(y_true[mask] - y_pred[mask]) / denom[mask]) * 100.0\n",
        "\n",
        "def build_supervised_from_series(y_series, extra_X=None, max_lag=MAX_LAG):\n",
        "    \"\"\"Create lagged features and rolling stats\"\"\"\n",
        "    df_feat = pd.DataFrame(index=y_series.index)\n",
        "    df_feat[\"y\"] = y_series.values\n",
        "\n",
        "    # Lag features\n",
        "    for L in range(1, max_lag+1):\n",
        "        df_feat[f\"lag_{L}\"] = y_series.shift(L)\n",
        "\n",
        "    # Rolling statistics (adapt to training window)\n",
        "    roll_windows = [3, min(6, max_lag)]\n",
        "    for w in roll_windows:\n",
        "        df_feat[f\"roll_mean_{w}\"] = y_series.shift(1).rolling(w).mean()\n",
        "        df_feat[f\"roll_std_{w}\"]  = y_series.shift(1).rolling(w).std()\n",
        "\n",
        "    # Calendar features\n",
        "    df_feat[\"month\"] = df_feat.index.month\n",
        "    df_feat[\"quarter\"] = df_feat.index.quarter\n",
        "\n",
        "    if extra_X is not None:\n",
        "        df_feat = df_feat.join(extra_X, how=\"left\")\n",
        "\n",
        "    # NOTE: We do NOT dropna() here. NaNs will be handled during splitting/forecasting.\n",
        "    return df_feat.drop(columns=[\"y\"]), df_feat[\"y\"]\n",
        "\n",
        "def rolling_window_splits(y_index, window=TRAIN_WINDOW, fh=FH, n_splits=N_SPLITS):\n",
        "    \"\"\"\n",
        "    Fixed-length rolling window CV\n",
        "    Ensures all folds are valid and evenly spaced\n",
        "    \"\"\"\n",
        "    n = len(y_index)\n",
        "\n",
        "    # Ensure we have enough data\n",
        "    if n < window + fh:\n",
        "        raise ValueError(f\"Need at least {window + fh} months, have {n}\")\n",
        "\n",
        "    # Calculate valid range for test set end positions\n",
        "    first_test_end = window + fh\n",
        "    last_test_end = n\n",
        "\n",
        "    if n_splits == 1:\n",
        "        # Single fold: use the very last possible window\n",
        "        test_end_positions = [last_test_end]\n",
        "    else:\n",
        "        # Multiple folds: evenly space them\n",
        "        test_end_positions = np.linspace(first_test_end, last_test_end, n_splits, dtype=int)\n",
        "\n",
        "    for test_end in test_end_positions:\n",
        "        train_start = test_end - fh - window\n",
        "        train_end = test_end - fh\n",
        "        test_start = train_end\n",
        "        test_end_ = test_end\n",
        "\n",
        "        train_idx = np.arange(train_start, train_end)\n",
        "        test_idx  = np.arange(test_start, test_end_)\n",
        "\n",
        "        yield train_idx, test_idx\n",
        "\n",
        "# ============================================\n",
        "# MODEL EVALUATION\n",
        "# ============================================\n",
        "\n",
        "def evaluate_models_auto_cv(y_series, X_exog=None, window=TRAIN_WINDOW, fh=FH, n_splits=N_SPLITS):\n",
        "    \"\"\"Evaluate all models with auto-adjusted CV\"\"\"\n",
        "    results = []\n",
        "    idx = y_series.index\n",
        "\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"üî¨ CROSS-VALIDATION\")\n",
        "    print(f\"{'='*60}\")\n",
        "    print(f\"Configuration: {n_splits} folds, train={window} months, forecast={fh} month\\n\")\n",
        "\n",
        "    # Pre-build full feature table - don't dropna here\n",
        "    X_full, y_full = build_supervised_from_series(y_series, extra_X=X_exog, max_lag=MAX_LAG)\n",
        "\n",
        "    # Add baseline (12-month average) for comparison\n",
        "    print(\"Models to evaluate:\")\n",
        "    print(\"  ‚Ä¢ Baseline (12-month moving average)\")\n",
        "    print(\"  ‚Ä¢ ARIMA\")\n",
        "    print(\"  ‚Ä¢ Ridge Regression\")\n",
        "    print(\"  ‚Ä¢ ElasticNet\")\n",
        "    print(\"  ‚Ä¢ XGBoost\")\n",
        "    print(\"  ‚Ä¢ LightGBM\\n\")\n",
        "\n",
        "    for fold, (tr, te) in enumerate(rolling_window_splits(idx, window=window, fh=fh, n_splits=n_splits), start=1):\n",
        "        train_dates = f\"{idx[tr[0]].date()} to {idx[tr[-1]].date()}\"\n",
        "        test_date = idx[te[0]].date()\n",
        "        print(f\"Fold {fold}/{n_splits}: Train [{train_dates}] ‚Üí Test [{test_date}]\")\n",
        "\n",
        "        y_tr = y_series.iloc[tr]\n",
        "        y_te = y_series.iloc[te]\n",
        "        test_index = idx[te]\n",
        "\n",
        "        # -----------------------\n",
        "        # BASELINE: 12-month (or window) average\n",
        "        # -----------------------\n",
        "        baseline_window = min(12, len(y_tr))\n",
        "        baseline_pred = y_tr.iloc[-baseline_window:].mean()\n",
        "        results.append([\"Baseline_MA\", fold,\n",
        "                       float(np.sqrt((y_te.values[0] - baseline_pred)**2)),\n",
        "                       float(np.abs(y_te.values[0] - baseline_pred)),\n",
        "                       float(smape(y_te.values, np.array([baseline_pred])))])\n",
        "\n",
        "        # -----------------------\n",
        "        # ARIMA\n",
        "        # -----------------------\n",
        "        try:\n",
        "            arima = pmd.auto_arima(y_tr, seasonal=False, stepwise=True,\n",
        "                                   suppress_warnings=True, error_action=\"ignore\",\n",
        "                                   max_p=3, max_q=3)  # Limit complexity for small data\n",
        "            y_pred = arima.predict(n_periods=fh)[0]\n",
        "            results.append([\"ARIMA\", fold,\n",
        "                            float(np.sqrt((y_te.values[0] - y_pred)**2)),\n",
        "                            float(np.abs(y_te.values[0] - y_pred)),\n",
        "                            float(smape(y_te.values, np.array([y_pred])))])\n",
        "        except Exception:\n",
        "            results.append([\"ARIMA\", fold, np.nan, np.nan, np.nan])\n",
        "\n",
        "        # -----------------------\n",
        "        # ML MODELS (with lag features)\n",
        "        # -----------------------\n",
        "        # Select training and test data after building full table\n",
        "        X_tr = X_full.iloc[tr].dropna() # Drop NaNs for training\n",
        "        y_tr_full = y_full.iloc[tr].loc[X_tr.index] # Align y_full with X_tr after dropping NaNs\n",
        "        X_te = X_full.iloc[te] # Keep NaNs in test for prediction\n",
        "\n",
        "        # Check if we have enough training data for ML models\n",
        "        min_ml_rows = max(5, MAX_LAG)\n",
        "\n",
        "        if len(X_te) >= 1 and len(X_tr) >= min_ml_rows:\n",
        "            # Ridge\n",
        "            try:\n",
        "                ridge = Pipeline([\n",
        "                    (\"scaler\", StandardScaler(with_mean=True, with_std=True)),\n",
        "                    (\"model\", Ridge(alpha=1.0))\n",
        "                ])\n",
        "                ridge.fit(X_tr, y_tr_full)\n",
        "                # Predict on test data, which might contain NaNs for future lags\n",
        "                y_pred = ridge.predict(X_te)[0]\n",
        "                results.append([\"Ridge\", fold,\n",
        "                                float(np.sqrt((y_te.values[0] - y_pred)**2)),\n",
        "                                float(np.abs(y_te.values[0] - y_pred)),\n",
        "                                float(smape(y_te.values, np.array([y_pred])))])\n",
        "            except Exception:\n",
        "                results.append([\"Ridge\", fold, np.nan, np.nan, np.nan])\n",
        "\n",
        "            # ElasticNet\n",
        "            try:\n",
        "                enet = Pipeline([\n",
        "                    (\"scaler\", StandardScaler(with_mean=True, with_std=True)),\n",
        "                    (\"model\", ElasticNet(alpha=0.1, l1_ratio=0.5, random_state=42, max_iter=10000))\n",
        "                ])\n",
        "                enet.fit(X_tr, y_tr_full)\n",
        "                y_pred = enet.predict(X_te)[0]\n",
        "                results.append([\"ElasticNet\", fold,\n",
        "                                float(np.sqrt((y_te.values[0] - y_pred)**2)),\n",
        "                                float(np.abs(y_te.values[0] - y_pred)),\n",
        "                                float(smape(y_te.values, np.array([y_pred])))])\n",
        "            except Exception:\n",
        "                results.append([\"ElasticNet\", fold, np.nan, np.nan, np.nan])\n",
        "\n",
        "            # XGBoost\n",
        "            try:\n",
        "                xgb = XGBRegressor(n_estimators=300, max_depth=3, learning_rate=0.05,\n",
        "                                   subsample=0.9, colsample_bytree=0.9, random_state=42,\n",
        "                                   verbosity=0)\n",
        "                xgb.fit(X_tr, y_tr_full)\n",
        "                y_pred = xgb.predict(X_te)[0]\n",
        "                results.append([\"XGBoost\", fold,\n",
        "                                float(np.sqrt((y_te.values[0] - y_pred)**2)),\n",
        "                                float(np.abs(y_te.values[0] - y_pred)),\n",
        "                                float(smape(y_te.values, np.array([y_pred])))])\n",
        "            except Exception:\n",
        "                results.append([\"XGBoost\", fold, np.nan, np.nan, np.nan])\n",
        "\n",
        "            # LightGBM\n",
        "            try:\n",
        "                lgbm = LGBMRegressor(n_estimators=300, num_leaves=15, learning_rate=0.05,\n",
        "                                     subsample=0.9, colsample_bytree=0.9, random_state=42,\n",
        "                                     verbose=-1)\n",
        "                lgbm.fit(X_tr, y_tr_full)\n",
        "                y_pred = lgbm.predict(X_te)[0]\n",
        "                results.append([\"LightGBM\", fold,\n",
        "                                float(np.sqrt((y_te.values[0] - y_pred)**2)),\n",
        "                                float(np.abs(y_te.values[0] - y_pred)),\n",
        "                                float(smape(y_te.values, np.array([y_pred])))])\n",
        "            except Exception:\n",
        "                results.append([\"LightGBM\", fold, np.nan, np.nan, np.nan])\n",
        "        else:\n",
        "            # Not enough rows for ML\n",
        "            for name in [\"Ridge\", \"ElasticNet\", \"XGBoost\", \"LightGBM\"]:\n",
        "                results.append([name, fold, np.nan, np.nan, np.nan])\n",
        "\n",
        "\n",
        "    res_df = pd.DataFrame(results, columns=[\"Model\",\"Fold\",\"RMSE\",\"MAE\",\"sMAPE\"])\n",
        "\n",
        "    # Calculate average metrics, handling NaNs\n",
        "    leaderboard = (res_df.groupby(\"Model\", as_index=True)\n",
        "                        .agg(RMSE=(\"RMSE\",\"mean\"),\n",
        "                             MAE=(\"MAE\",\"mean\"),\n",
        "                             sMAPE=(\"sMAPE\",\"mean\"),\n",
        "                             Valid_Folds=(\"RMSE\", lambda x: x.notna().sum()))\n",
        "                        .sort_values(\"RMSE\"))\n",
        "\n",
        "    return res_df, leaderboard\n",
        "\n",
        "# ============================================\n",
        "# RUN EVALUATION\n",
        "# ============================================\n",
        "\n",
        "cv_results, leaderboard = evaluate_models_auto_cv(y, X_exog=X_exog,\n",
        "                                                   window=TRAIN_WINDOW, fh=FH, n_splits=N_SPLITS)\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"üèÜ FINAL LEADERBOARD (Lower is Better)\")\n",
        "print(\"=\"*60)\n",
        "print(leaderboard.to_string())\n",
        "print(\"\\n\")\n",
        "\n",
        "# Show which models completed successfully\n",
        "valid_models = leaderboard[leaderboard['Valid_Folds'] == N_SPLITS]\n",
        "if len(valid_models) > 0:\n",
        "    print(f\"‚úÖ Models with all {N_SPLITS} successful folds:\")\n",
        "    print(f\"   {', '.join(valid_models.index.tolist())}\")\n",
        "else:\n",
        "    print(f\"‚ö†Ô∏è No models completed all {N_SPLITS} folds successfully\")\n",
        "\n",
        "# ============================================\n",
        "# FEATURE IMPORTANCE (if winner is ML model)\n",
        "# ============================================\n",
        "\n",
        "best_model = leaderboard.index[0]\n",
        "if best_model in [\"XGBoost\", \"LightGBM\", \"Ridge\", \"ElasticNet\"]:\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"üîç FEATURE IMPORTANCE FOR {best_model}\")\n",
        "    print(f\"{'='*60}\")\n",
        "\n",
        "    # Re-build full feature table - don't dropna here\n",
        "    X_full, y_full = build_supervised_from_series(y, extra_X=X_exog, max_lag=MAX_LAG)\n",
        "\n",
        "    # Use last training window for feature importance, dropna for training\n",
        "    last_window_idx = y.index[-TRAIN_WINDOW:]\n",
        "    X_tr = X_full.loc[last_window_idx].dropna()\n",
        "    y_tr = y_full.loc[X_tr.index] # Align y_full with X_tr\n",
        "\n",
        "    if best_model == \"XGBoost\":\n",
        "        model = XGBRegressor(n_estimators=300, max_depth=3, learning_rate=0.05,\n",
        "                            subsample=0.9, colsample_bytree=0.9, random_state=42, verbosity=0)\n",
        "    elif best_model == \"LightGBM\":\n",
        "        model = LGBMRegressor(n_estimators=300, num_leaves=15, learning_rate=0.05,\n",
        "                             subsample=0.9, colsample_bytree=0.9, random_state=42, verbose=-1)\n",
        "    elif best_model == \"Ridge\":\n",
        "        model = Pipeline([\n",
        "            (\"scaler\", StandardScaler(with_mean=True, with_std=True)),\n",
        "            (\"model\", Ridge(alpha=1.0))\n",
        "        ])\n",
        "    else:  # ElasticNet\n",
        "        model = Pipeline([\n",
        "            (\"scaler\", StandardScaler(with_mean=True, with_std=True)),\n",
        "            (\"model\", ElasticNet(alpha=0.1, l1_ratio=0.5, random_state=42, max_iter=10000))\n",
        "        ])\n",
        "\n",
        "    model.fit(X_tr, y_tr)\n",
        "\n",
        "    # Extract feature importance\n",
        "    if best_model in [\"XGBoost\", \"LightGBM\"]:\n",
        "        feat_imp = pd.DataFrame({\n",
        "            'Feature': X_tr.columns, # Use X_tr columns\n",
        "            'Importance': model.feature_importances_\n",
        "        }).sort_values('Importance', ascending=False)\n",
        "    else:  # Ridge or ElasticNet\n",
        "        feat_imp = pd.DataFrame({\n",
        "            'Feature': X_tr.columns, # Use X_tr columns\n",
        "            'Coefficient': np.abs(model.named_steps['model'].coef_)\n",
        "        }).sort_values('Coefficient', ascending=False)\n",
        "        feat_imp.columns = ['Feature', 'Importance']\n",
        "\n",
        "    print(\"\\nTop 10 Most Important Features:\")\n",
        "    print(feat_imp.head(10).to_string(index=False))\n",
        "\n",
        "# ============================================\n",
        "# FINAL PREDICTION\n",
        "# ============================================\n",
        "\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(f\"üéØ FINAL FORECAST USING: {best_model}\")\n",
        "print(f\"{'='*60}\")\n",
        "\n",
        "last_window_idx = y.index[-TRAIN_WINDOW:]\n",
        "y_train_last = y.loc[last_window_idx]\n",
        "next_month = (y.index[-1] + pd.offsets.MonthBegin(1))\n",
        "\n",
        "if best_model == \"Baseline_MA\":\n",
        "    baseline_window = min(12, len(y))\n",
        "    yhat = float(y.iloc[-baseline_window:].mean())\n",
        "\n",
        "elif best_model == \"ARIMA\":\n",
        "    mdl = pmd.auto_arima(y_train_last, seasonal=False, stepwise=True,\n",
        "                         suppress_warnings=True, error_action=\"ignore\",\n",
        "                         max_p=3, max_q=3)\n",
        "    yhat = float(mdl.predict(n_periods=FH)[0])\n",
        "\n",
        "else:  # ML models\n",
        "    # Build features for the entire series (including the forecast month as NaN)\n",
        "    y_tmp = y.copy()\n",
        "    y_tmp.loc[next_month] = np.nan\n",
        "    X_full_for_pred, _ = build_supervised_from_series(y_tmp, extra_X=X_exog, max_lag=MAX_LAG)\n",
        "\n",
        "    # Select training data (dropna for training)\n",
        "    X_tr = X_full_for_pred.loc[last_window_idx].dropna()\n",
        "    y_tr_full = y.loc[X_tr.index] # y is the original series without the NaN forecast month\n",
        "\n",
        "    # Select the features for the next month (do NOT drop NaNs here, these are the values to predict)\n",
        "    X_next = X_full_for_pred.loc[[next_month]]\n",
        "\n",
        "    if best_model == \"Ridge\":\n",
        "        model = Pipeline([\n",
        "            (\"scaler\", StandardScaler(with_mean=True, with_std=True)),\n",
        "            (\"model\", Ridge(alpha=1.0))\n",
        "        ])\n",
        "    elif best_model == \"ElasticNet\":\n",
        "        model = Pipeline([\n",
        "            (\"scaler\", StandardScaler(with_mean=True, with_std=True)),\n",
        "            (\"model\", ElasticNet(alpha=0.1, l1_ratio=0.5, random_state=42, max_iter=10000))\n",
        "        ])\n",
        "    elif best_model == \"XGBoost\":\n",
        "        model = XGBRegressor(n_estimators=300, max_depth=3, learning_rate=0.05,\n",
        "                           subsample=0.9, colsample_bytree=0.9, random_state=42, verbosity=0)\n",
        "    else:  # LightGBM\n",
        "        model = LGBMRegressor(n_estimators=300, num_leaves=15, learning_rate=0.05,\n",
        "                            subsample=0.9, colsample_bytree=0.9, random_state=42, verbose=-1)\n",
        "\n",
        "    model.fit(X_tr, y_tr_full)\n",
        "    yhat = float(model.predict(X_next)[0])\n",
        "\n",
        "forecast = pd.DataFrame({\n",
        "    \"Month\": [next_month.strftime('%Y-%m')],\n",
        "    \"Predicted_Cost\": [f\"${yhat:,.2f}\"],\n",
        "    \"Model_Used\": [best_model],\n",
        "    \"Training_Window\": [f\"{y_train_last.index[0].strftime('%Y-%m')} to {y_train_last.index[-1].strftime('%Y-%m')}\"]\n",
        "})\n",
        "\n",
        "print(f\"\\n‚úÖ Forecast for {next_month.strftime('%B %Y')}:\")\n",
        "print(forecast.to_string(index=False))\n",
        "\n",
        "# ============================================\n",
        "# SAVE OUTPUTS\n",
        "# ============================================\n",
        "\n",
        "monthly.to_csv(\"monthly_approved_cost.csv\")\n",
        "forecast_save = pd.DataFrame({\n",
        "    \"Month\": [next_month],\n",
        "    \"Predicted_Cost\": [yhat],\n",
        "    \"Model_Used\": [best_model],\n",
        "    \"CV_Folds\": [N_SPLITS],\n",
        "    \"Training_Window_Months\": [TRAIN_WINDOW]\n",
        "})\n",
        "forecast_save.to_csv(\"next_month_forecast.csv\", index=False)\n",
        "\n",
        "# Save detailed CV results\n",
        "cv_results.to_csv(\"cv_results_detailed.csv\", index=False)\n",
        "leaderboard.to_csv(\"model_leaderboard.csv\")\n",
        "\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(\"üíæ SAVED FILES:\")\n",
        "print(\"  ‚úì monthly_approved_cost.csv\")\n",
        "print(\"  ‚úì next_month_forecast.csv\")\n",
        "print(\"  ‚úì cv_results_detailed.csv\")\n",
        "print(\"  ‚úì model_leaderboard.csv\")\n",
        "print(f\"{'='*60}\")\n",
        "print(\"\\n‚ú® Analysis complete!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FBebnqPb8AjF",
        "outputId": "5ff00957-24c1-4a62-e127-cf62a077a1b8"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Setup complete!\n",
            "\n",
            "üìÇ Looking for: Claims.xlsx\n",
            "\n",
            "üìä Loading data...\n",
            "‚úÖ Loaded 900,000 rows from Excel\n",
            "‚úÖ Order filter: kept 899,510, dropped 490 (invalid or TREATMENT > BATCH)\n",
            "\n",
            "üîç Available status codes:\n",
            "STATUS\n",
            "AC    667473\n",
            "RJ    231888\n",
            "SU       149\n",
            "Name: count, dtype: int64\n",
            "\n",
            "‚úÖ Filtered to 667,473 approved claims\n",
            "‚úÖ 667,473 claims with positive amounts\n",
            "\n",
            "============================================================\n",
            "üìÖ Monthly Data Summary\n",
            "============================================================\n",
            "Date range: 2023-01-01 ‚Üí 2023-12-01\n",
            "Total months: 12\n",
            "Total claims amount: $206,628,283\n",
            "Average monthly cost: $17,219,024\n",
            "\n",
            "Last 6 months:\n",
            "             TOTAL_COST\n",
            "_MONTH                 \n",
            "2023-07-01  16205185.07\n",
            "2023-08-01  16696400.64\n",
            "2023-09-01  16917400.96\n",
            "2023-10-01  18769256.95\n",
            "2023-11-01  17215728.24\n",
            "2023-12-01  18341687.91\n",
            "\n",
            "============================================================\n",
            "üîß AUTO-ADJUSTING CV PARAMETERS\n",
            "============================================================\n",
            "‚ÑπÔ∏è Moderate data: Adjusting TRAIN_WINDOW to 10 months\n",
            "‚úÖ MAX_LAG adjusted to 5 (‚â§ 10//2)\n",
            "‚ö†Ô∏è Using N_SPLITS = 2 fold (very limited data)\n",
            "\n",
            "üìä Final CV Configuration:\n",
            "   ‚Ä¢ Training window: 10 months\n",
            "   ‚Ä¢ Forecast horizon: 1 month\n",
            "   ‚Ä¢ CV folds: 2\n",
            "   ‚Ä¢ Max lag features: 5\n",
            "   ‚Ä¢ Total evaluations: 2 per model\n",
            "\n",
            "============================================================\n",
            "üî¨ CROSS-VALIDATION\n",
            "============================================================\n",
            "Configuration: 2 folds, train=10 months, forecast=1 month\n",
            "\n",
            "Models to evaluate:\n",
            "  ‚Ä¢ Baseline (12-month moving average)\n",
            "  ‚Ä¢ ARIMA\n",
            "  ‚Ä¢ Ridge Regression\n",
            "  ‚Ä¢ ElasticNet\n",
            "  ‚Ä¢ XGBoost\n",
            "  ‚Ä¢ LightGBM\n",
            "\n",
            "Fold 1/2: Train [2023-01-01 to 2023-10-01] ‚Üí Test [2023-11-01]\n",
            "Fold 2/2: Train [2023-02-01 to 2023-11-01] ‚Üí Test [2023-12-01]\n",
            "\n",
            "============================================================\n",
            "üèÜ FINAL LEADERBOARD (Lower is Better)\n",
            "============================================================\n",
            "                     RMSE           MAE     sMAPE  Valid_Folds\n",
            "Model                                                         \n",
            "XGBoost      1.714911e+05  1.714911e+05  0.960042            2\n",
            "Baseline_MA  6.884605e+05  6.884605e+05  3.897712            2\n",
            "LightGBM     8.279052e+05  8.279052e+05  4.716621            2\n",
            "Ridge        1.195079e+06  1.195079e+06  6.659374            2\n",
            "ARIMA        1.229585e+06  1.229585e+06  7.145923            2\n",
            "ElasticNet   1.280819e+06  1.280819e+06  7.130833            2\n",
            "\n",
            "\n",
            "‚úÖ Models with all 2 successful folds:\n",
            "   XGBoost, Baseline_MA, LightGBM, Ridge, ARIMA, ElasticNet\n",
            "\n",
            "============================================================\n",
            "üîç FEATURE IMPORTANCE FOR XGBoost\n",
            "============================================================\n",
            "\n",
            "Top 10 Most Important Features:\n",
            "    Feature  Importance\n",
            "      month    0.479653\n",
            " roll_std_5    0.171327\n",
            "      lag_4    0.149981\n",
            " roll_std_3    0.084195\n",
            "      lag_5    0.048333\n",
            "roll_mean_3    0.043258\n",
            "      lag_1    0.014469\n",
            "    quarter    0.007098\n",
            "      lag_2    0.001475\n",
            "      lag_3    0.000212\n",
            "\n",
            "============================================================\n",
            "üéØ FINAL FORECAST USING: XGBoost\n",
            "============================================================\n",
            "\n",
            "‚úÖ Forecast for January 2024:\n",
            "  Month Predicted_Cost Model_Used    Training_Window\n",
            "2024-01 $17,966,530.00    XGBoost 2023-03 to 2023-12\n",
            "\n",
            "============================================================\n",
            "üíæ SAVED FILES:\n",
            "  ‚úì monthly_approved_cost.csv\n",
            "  ‚úì next_month_forecast.csv\n",
            "  ‚úì cv_results_detailed.csv\n",
            "  ‚úì model_leaderboard.csv\n",
            "============================================================\n",
            "\n",
            "‚ú® Analysis complete!\n"
          ]
        }
      ]
    }
  ]
}